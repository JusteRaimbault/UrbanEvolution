"service_id"))
calendar <- calendar[, c("service_id_new", "monday",
"tuesday", "wednesday", "thursday", "friday", "saturday",
"sunday", "start_date", "end_date")]
names(calendar) <- c("service_id", "monday", "tuesday",
"wednesday", "thursday", "friday", "saturday", "sunday",
"start_date", "end_date")
if (nrow(calendar_dates) > 0) {
calendar_dates <- dplyr::left_join(calendar_dates,
service_id, by = c("file_id", "service_id"))
calendar_dates <- calendar_dates[, c("service_id_new",
"date", "exception_type")]
names(calendar_dates) <- c("service_id", "date",
"exception_type")
}
}
if (any(duplicated(trips$trip_id))) {
message("De-duplicating trip_id")
trip_id <- trips[, c("file_id", "trip_id")]
if (any(duplicated(trip_id))) {
stop("Duplicated trip_id within the same GTFS file")
}
trip_id$trip_id_new <- seq(1, nrow(trip_id))
trips <- dplyr::left_join(trips, trip_id, by = c("file_id",
"trip_id"))
trips <- trips[, c("route_id", "service_id", "trip_id_new",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
stop_times <- dplyr::left_join(stop_times, trip_id, by = c("file_id",
"trip_id"))
stop_times <- stop_times[, c("trip_id_new", "arrival_time",
"departure_time", "stop_id", "stop_sequence", "timepoint")]
names(stop_times) <- c("trip_id", "arrival_time", "departure_time",
"stop_id", "stop_sequence", "timepoint")
}
if (exists("service_id")) {
trips <- dplyr::left_join(trips, service_id, by = c("file_id",
"service_id"))
trips <- trips[, c("route_id", "service_id_new", "trip_id",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
}
if (exists("route_id")) {
trips <- dplyr::left_join(trips, route_id, by = c("file_id",
"route_id"))
trips <- trips[, c("route_id_new", "service_id", "trip_id",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
}
trips <- trips[, c("route_id", "service_id", "trip_id")]
names(trips) <- c("route_id", "service_id", "trip_id")
if (nrow(calendar_dates) > 0) {
message("Condensing duplicated service patterns")
calendar_dates_summary <- dplyr::group_by(calendar_dates,
service_id)
if (class(calendar_dates_summary$date) == "Date") {
calendar_dates_summary <- dplyr::summarise(calendar_dates_summary,
pattern = paste(c(as.character(date), exception_type),
collapse = ""))
}
else {
calendar_dates_summary <- dplyr::summarise(calendar_dates_summary,
pattern = paste(c(date, exception_type), collapse = ""))
}
calendar_summary <- dplyr::left_join(calendar, calendar_dates_summary,
by = "service_id")
calendar_summary <- dplyr::group_by(calendar_summary,
start_date, end_date, monday, tuesday, wednesday,
thursday, friday, saturday, sunday, pattern)
calendar_summary$service_id_new <- dplyr::group_indices(calendar_summary)
calendar_summary <- calendar_summary[, c("service_id_new",
"service_id")]
trips <- dplyr::left_join(trips, calendar_summary, by = c("service_id"))
trips <- trips[, c("route_id", "service_id_new", "trip_id")]
names(trips) <- c("route_id", "service_id", "trip_id")
calendar <- dplyr::left_join(calendar, calendar_summary,
by = c("service_id"))
calendar <- calendar[, c("service_id_new", "monday",
"tuesday", "wednesday", "thursday", "friday", "saturday",
"sunday", "start_date", "end_date")]
names(calendar) <- c("service_id", "monday", "tuesday",
"wednesday", "thursday", "friday", "saturday", "sunday",
"start_date", "end_date")
calendar <- calendar[!duplicated(calendar$service_id),
]
calendar_dates <- dplyr::left_join(calendar_dates, calendar_summary,
by = c("service_id"))
calendar_dates <- calendar_dates[, c("service_id_new",
"date", "exception_type")]
names(calendar_dates) <- c("service_id", "date", "exception_type")
calendar_dates <- calendar_dates[!duplicated(calendar_dates$service_id),
]
}
stop_times$file_id <- NULL
routes$file_id <- NULL
calendar$file_id <- NULL
res_final <- list(agency, stops, routes, trips, stop_times,
calendar, calendar_dates)
names(res_final) <- c("agency", "stops", "routes", "trips",
"stop_times", "calendar", "calendar_dates")
return(res_final)
}
res = gtfs_merge_force(cleandata, force=T)
gtfs_merge_force <- function (gtfs_list, force = FALSE)
{
gtfs_list <- gtfs_list[lengths(gtfs_list) != 0]
agency <- sapply(gtfs_list, "[", "agency")
stops <- sapply(gtfs_list, "[", "stops")
routes <- sapply(gtfs_list, "[", "routes")
trips <- sapply(gtfs_list, "[", "trips")
stop_times <- sapply(gtfs_list, "[", "stop_times")
calendar <- sapply(gtfs_list, "[", "calendar")
calendar_dates <- sapply(gtfs_list, "[", "calendar_dates")
names(agency) <- seq(1, length(agency))
suppressWarnings(agency <- dplyr::bind_rows(agency, .id = "file_id"))
names(stops) <- seq(1, length(stops))
suppressWarnings(stops <- dplyr::bind_rows(stops, .id = "file_id"))
names(routes) <- seq(1, length(routes))
suppressWarnings(routes <- dplyr::bind_rows(routes, .id = "file_id"))
names(trips) <- seq(1, length(trips))
suppressWarnings(trips <- dplyr::bind_rows(trips, .id = "file_id"))
names(stop_times) <- seq(1, length(stop_times))
suppressWarnings(stop_times <- dplyr::bind_rows(stop_times,
.id = "file_id"))
names(calendar) <- seq(1, length(calendar))
suppressWarnings(calendar <- dplyr::bind_rows(calendar, .id = "file_id"))
names(calendar_dates) <- seq(1, length(calendar_dates))
calendar_dates <- calendar_dates[sapply(calendar_dates, nrow) >
0]
suppressWarnings(calendar_dates <- dplyr::bind_rows(calendar_dates,
.id = "file_id"))
agency$agency_name <- as.character(agency$agency_name)
agency$agency_name[agency$agency_name == "Dockland Light Railway"] <- "Docklands Light Railway"
agency$agency_name[agency$agency_name == "Edward Bros"] <- "Edwards Bros"
agency$agency_name[agency$agency_name == "John`s Coaches"] <- "John's Coaches"
agency$agency_name[agency$agency_name == "Stagecoach in Lancaster."] <- "Stagecoach in Lancashire"
agency$agency_id[agency$agency_name == "Tanat Valley Coaches"] <- "TanVaCo"
if (any(agency$agency_name == agency$agency_id)) {
agency_sub <- agency
agency_sub$file_id <- NULL
agency_sub <- unique(agency)
id_dups <- agency_sub$agency_id[duplicated(agency_sub$agency_id)]
if (length(id_dups) > 0) {
agency_sub <- agency_sub[agency_sub$agency_id %in%
id_dups, ]
agency_sub <- agency_sub[agency_sub$agency_id !=
agency_sub$agency_name, ]
for (i in seq(1, nrow(agency_sub))) {
agency$agency_name[agency$agency_name == agency_sub$agency_id[i]] <- agency_sub$agency_name[i]
}
}
else {
rm(agency_sub, id_dups)
}
}
agency$file_id <- NULL
agency <- unique(agency)
if (any(duplicated(agency$agency_id))) {
agency.check <- agency
agency.check$agency_name <- tolower(agency.check$agency_name)
agency.check <- unique(agency.check)
if (any(duplicated(agency.check$agency_id))) {
if (force) {
warning(paste0("Duplicated Agency IDs ", paste(unique(agency.check$agency_id[duplicated(agency.check$agency_id)]),
collapse = " "), " will be merged"))
agency <- dplyr::group_by(agency, agency_id)
agency <- dplyr::summarise(agency, agency_name = agency_name[1],
agency_url = agency_url[1], agency_timezone = agency_timezone[1],
agency_lang = agency_lang[1])
}
else {
stop(paste0("Duplicated Agency IDs ", paste(unique(agency.check$agency_id[duplicated(agency.check$agency_id)]),
collapse = " ")))
}
}
else {
agency <- agency[!duplicated(agency$agency_id), ]
}
}
stops$file_id <- NULL
stops <- unique(stops)
# ! commented: finds duplicates although None: ??? - issue with different version of dplyr? - tidyverse is a bloody mess
#if (any(duplicated(stops$stop_id))) {
#  stop("Duplicated Stop IDS")
#}
if (any(duplicated(routes$route_id))) {
message("De-duplicating route_id")
route_id <- routes[, c("file_id", "route_id")]
if (any(duplicated(route_id))) {
if (force) {
routes <- routes[!duplicated(route_id), ]
}
else {
stop("Duplicated route_id within the same GTFS file, try using force = TRUE")
}
}
route_id$route_id_new <- seq(1, nrow(route_id))
routes <- dplyr::left_join(routes, route_id, by = c("file_id",
"route_id"))
routes <- routes[, c("route_id_new", "agency_id", "route_short_name",
"route_long_name", "route_desc", "route_type")]
names(routes) <- c("route_id", "agency_id", "route_short_name",
"route_long_name", "route_desc", "route_type")
}
if (any(duplicated(calendar$service_id))) {
message("De-duplicating service_id")
service_id <- calendar[, c("file_id", "service_id")]
if (any(duplicated(service_id))) {
stop("Duplicated service_id within the same GTFS file")
}
service_id$service_id_new <- seq(1, nrow(service_id))
calendar <- dplyr::left_join(calendar, service_id, by = c("file_id",
"service_id"))
calendar <- calendar[, c("service_id_new", "monday",
"tuesday", "wednesday", "thursday", "friday", "saturday",
"sunday", "start_date", "end_date")]
names(calendar) <- c("service_id", "monday", "tuesday",
"wednesday", "thursday", "friday", "saturday", "sunday",
"start_date", "end_date")
if (nrow(calendar_dates) > 0) {
calendar_dates <- dplyr::left_join(calendar_dates,
service_id, by = c("file_id", "service_id"))
calendar_dates <- calendar_dates[, c("service_id_new",
"date", "exception_type")]
names(calendar_dates) <- c("service_id", "date",
"exception_type")
}
}
if (any(duplicated(trips$trip_id))) {
message("De-duplicating trip_id")
trip_id <- trips[, c("file_id", "trip_id")]
if (any(duplicated(trip_id))) {
stop("Duplicated trip_id within the same GTFS file")
}
trip_id$trip_id_new <- seq(1, nrow(trip_id))
trips <- dplyr::left_join(trips, trip_id, by = c("file_id",
"trip_id"))
trips <- trips[, c("route_id", "service_id", "trip_id_new",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
stop_times <- dplyr::left_join(stop_times, trip_id, by = c("file_id",
"trip_id"))
stop_times <- stop_times[, c("trip_id_new", "arrival_time",
"departure_time", "stop_id", "stop_sequence", "timepoint")]
names(stop_times) <- c("trip_id", "arrival_time", "departure_time",
"stop_id", "stop_sequence", "timepoint")
}
if (exists("service_id")) {
trips <- dplyr::left_join(trips, service_id, by = c("file_id",
"service_id"))
trips <- trips[, c("route_id", "service_id_new", "trip_id",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
}
if (exists("route_id")) {
trips <- dplyr::left_join(trips, route_id, by = c("file_id",
"route_id"))
trips <- trips[, c("route_id_new", "service_id", "trip_id",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
}
trips <- trips[, c("route_id", "service_id", "trip_id")]
names(trips) <- c("route_id", "service_id", "trip_id")
if (nrow(calendar_dates) > 0) {
message("Condensing duplicated service patterns")
calendar_dates_summary <- dplyr::group_by(calendar_dates,
service_id)
if (class(calendar_dates_summary$date) == "Date") {
calendar_dates_summary <- dplyr::summarise(calendar_dates_summary,
pattern = paste(c(as.character(date), exception_type),
collapse = ""))
}
else {
calendar_dates_summary <- dplyr::summarise(calendar_dates_summary,
pattern = paste(c(date, exception_type), collapse = ""))
}
calendar_summary <- dplyr::left_join(calendar, calendar_dates_summary,
by = "service_id")
calendar_summary <- dplyr::group_by(calendar_summary,
start_date, end_date, monday, tuesday, wednesday,
thursday, friday, saturday, sunday, pattern)
calendar_summary$service_id_new <- dplyr::group_indices(calendar_summary)
calendar_summary <- calendar_summary[, c("service_id_new",
"service_id")]
trips <- dplyr::left_join(trips, calendar_summary, by = c("service_id"))
trips <- trips[, c("route_id", "service_id_new", "trip_id")]
names(trips) <- c("route_id", "service_id", "trip_id")
calendar <- dplyr::left_join(calendar, calendar_summary,
by = c("service_id"))
calendar <- calendar[, c("service_id_new", "monday",
"tuesday", "wednesday", "thursday", "friday", "saturday",
"sunday", "start_date", "end_date")]
names(calendar) <- c("service_id", "monday", "tuesday",
"wednesday", "thursday", "friday", "saturday", "sunday",
"start_date", "end_date")
calendar <- calendar[!duplicated(calendar$service_id),
]
calendar_dates <- dplyr::left_join(calendar_dates, calendar_summary,
by = c("service_id"))
calendar_dates <- calendar_dates[, c("service_id_new",
"date", "exception_type")]
names(calendar_dates) <- c("service_id", "date", "exception_type")
calendar_dates <- calendar_dates[!duplicated(calendar_dates$service_id),
]
}
stop_times$file_id <- NULL
routes$file_id <- NULL
calendar$file_id <- NULL
res_final <- list(agency, stops, routes, trips, stop_times,
calendar, calendar_dates)
names(res_final) <- c("agency", "stops", "routes", "trips",
"stop_times", "calendar", "calendar_dates")
return(res_final)
}
stopids = list()
for(i in 1:2){#1:length(datalist)){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
# Error duplicated agencies Duplicated Agency IDs 1 3 DC 2 7778462 TEL CTG RRS RBUS 4 A2BR ARBB CX EB GLAR HCC MN MT O2 SK SV WCT ATL CAR DGC JOH THC
# -> force = T
res = gtfs_merge_force(cleandata, force=T)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
res = gtfs_merge_force(cleandata, force=T)
res
merge_all_gtfs<-function(datadir,region_codes = c('EA','EM','L','NE','NW','S','SE','SW','W','WM'),files=paste0(region_codes,'_gtfs.zip')){
datalist = lapply(files,function(f){return(gtfs_read(paste0(datadir,f)))})
names(datalist) <-region_codes
cleandata = datalist
# some cleaning to do: gtfs_merge has bugs
# names(datalist[[1]])
# sapply(datalist[[1]],names)
# Column `stop_id` can't be converted from character to numeric
# datalist[[1]][["stops"]][["stop_id"]] # -> numeric ids !: use hash?
# sapply(md5(datalist[[1]][["stops"]][["stop_id"]]),hex_to_int)
#  + store new ids in a HashSet for stop times
# ! duplicated stop ids -> force does not work: concatenate region code
stopids = list()
for(i in 1:length(datalist)){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
# Error duplicated agencies Duplicated Agency IDs 1 3 DC 2 7778462 TEL CTG RRS RBUS 4 A2BR ARBB CX EB GLAR HCC MN MT O2 SK SV WCT ATL CAR DGC JOH THC
# -> force = T
res = gtfs_merge_force(cleandata, force=T)
gtfs_write(res,folder = datadir,name='all_gtfs')
}
targetdir
merge_all_gtfs(targetdir)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
merge_all_gtfs
merge_all_gtfs(targetdir)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
stopids = list()
for(i in 1:length(datalist)){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
show(length(rawids))
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
rawids
datalist[[i]][["stops"]][["stop_id"]])
datalist[[i]][["stops"]][["stop_id"]]
datalist[[i]][["stops"]]
nrow(datalist[[i]][["stops"]])
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
stopids = list()
for(i in 1:length(datalist)){
if(nrow(datalist[[i]][["stops"]])>0){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
show(length(rawids))
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
}
res = gtfs_merge_force(cleandata, force=T)
res
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
res
merge_all_gtfs(targetdir)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
setwd(paste0(Sys.getenv('CS_HOME'),'/UrbanEvolution/Models/EvolutionInnovation'))
library(ggplot2)
library(dplyr)
source(paste0(Sys.getenv('CS_HOME'),'/Organisation/Models/Utils/R/plots.R'))
res <- as.tbl(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population1000.csv'))
res <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population1000.csv'))
res
res$oppAverageUtility
res$oppAverageUtility.1
res$averageGravityFlow
res$averageGravityFlow.1
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
g+geom_point(alpha=0.5)
res <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population2000.csv'))
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
g+geom_point(alpha=0.5)
res <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population3000.csv'))
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
res <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population3000.csv'))
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
g+geom_point(alpha=0.5)
res <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population6000.csv'))
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
g+geom_point(alpha=0.5)
nammes(res)
names(res)
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=newInnovationHierarchy, size = innovationDecay))
g+geom_point(alpha=0.5)
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=mutationRate, size = innovationDecay))
g+geom_point(alpha=0.5)
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=newInnovationHierarchy, size = innovationDecay))
g+geom_point(alpha=0.5)
table(res$utilityDistribution)
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=newInnovationHierarchy, size = innovationDecay, shape=utilityDistribution))
g+geom_point(alpha=0.5)
names(res)
res <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_LOCAL_20211129_115043/population10000.csv'))
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay))
g+geom_point(alpha=0.5)
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=newInnovationHierarchy, size = innovationDecay, shape=utilityDistribution))
g+geom_point(alpha=0.5)
## two Pareto fronts
res1 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_0_5_LOCAL_20211129_165201/population3000.csv'))
allres = rbind(cbind(res,hierarchy=rep("1",nrow(res))),cbind(res1,hierarchy=rep("0.5",nrow(res1))),cbind(res2,hierarchy=rep("1.5",nrow(res2))))
## two Pareto fronts
res1 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_0_5_LOCAL_20211129_165201/population3000.csv'))
res2 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_1_5_LOCAL_20211129_165057/population3000.csv'))
allres = rbind(cbind(res,hierarchy=rep("1",nrow(res))),cbind(res1,hierarchy=rep("0.5",nrow(res1))),cbind(res2,hierarchy=rep("1.5",nrow(res2))))
g <- ggplot(allres,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay, shape=hierarchy))
g+geom_point(alpha=0.5)
## several Pareto fronts: initial hierarchy
res1 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_0_5_LOCAL_20211129_165201/population10000.csv'))
res2 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_1_5_LOCAL_20211129_165057/population10000.csv'))
allres = rbind(cbind(res,hierarchy=rep("1",nrow(res))),cbind(res1,hierarchy=rep("0.5",nrow(res1))),cbind(res2,hierarchy=rep("1.5",nrow(res2))))
g <- ggplot(allres,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay, shape=hierarchy))
g+geom_point(alpha=0.5)
g <- ggplot(res,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay, shape=utilityDistribution))
g+geom_point(alpha=0.5)
resprefix = 'CALIBRATION_REPLICATIONS_LOCAL_20211129_115043'
res <- as_tibble(read.csv(paste0('openmole/calibration/',resprefix,'/population10000.csv')))
resdir = paste0(Sys.getenv('CS_HOME'),'/UrbanEvolution/Results/EvolutionInnovation',resprefix,'/')
resdir
resdir = paste0(Sys.getenv('CS_HOME'),'/UrbanEvolution/Results/EvolutionInnovation/',resprefix,'/');dir.create(resdir)
ggsave(file=paste0(resdir,'pareto-oppAverageUtility-averageGravityFlow_color-gravityDecay_size-innovationDecay_shape-utilityDistribution.png'),width=20,height=18,units='cm')
ggsave(file=paste0(resdir,'pareto-oppAverageUtility-averageGravityFlow_color-gravityDecay_size-innovationDecay_shape-utilityDistribution.png'),width=22,height=18,units='cm')
## several Pareto fronts: initial hierarchy
res1 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_0_5_LOCAL_20211129_165201/population10000.csv'))
res2 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_HIERARCHY_1_5_LOCAL_20211129_165057/population10000.csv'))
allres = rbind(cbind(res,hierarchy=rep("1",nrow(res))),cbind(res1,hierarchy=rep("0.5",nrow(res1))),cbind(res2,hierarchy=rep("1.5",nrow(res2))))
g <- ggplot(allres,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay, shape=hierarchy))
g+geom_point(alpha=0.5)
ggsave(file=paste0(resdir,'pareto-oppAverageUtility-averageGravityFlow_VARYINGHIERARCHY_color-gravityDecay_size-innovationDecay.png'),width=22,height=18,units='cm')
## innovation hierarchy
res1 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_INNOVHIERARCHY_0_5_LOCAL_20211129_205704//population10000.csv'))
res2 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_INNOVHIERARCHY_1_LOCAL_20211129_221145//population10000.csv'))
res3 <- as_tibble(read.csv('openmole/calibration/CALIBRATION_REPLICATIONS_INNOVHIERARCHY_1_5_LOCAL_20211129_214916///population10000.csv'))
allres = rbind(cbind(res2,innovHierarchy=rep("1",nrow(res2))),cbind(res1,innovHierarchy=rep("0.5",nrow(res1))),cbind(res3,innovHierarchy=rep("1.5",nrow(res3))))
g <- ggplot(allres,aes(x=oppAverageUtility, y = averageGravityFlow, color=gravityDecay, size = innovationDecay, shape=innovHierarchy))
g+geom_point(alpha=0.5)
ggsave(file=paste0(resdir,'pareto-oppAverageUtility-averageGravityFlow_VARYINGINNOVHIERARCHY_color-gravityDecay_size-innovationDecay.png'),width=22,height=18,units='cm')
